{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWudKQoCMg8w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VC_Zgu0sNLHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical Questions\n",
        "1. What is unsupervised learning in the context of machine learning?\n",
        "\n",
        "Unsupervised learning is a machine learning approach where the model learns patterns from unlabeled data without predefined outputs. It identifies structures, such as clusters or associations, in the data.\n",
        "\n",
        "2. How does K-Means clustering algorithm work?\n",
        "\n",
        "K-Means partitions data into K clusters by:\n",
        "\n",
        "Randomly initializing K centroids.\n",
        "Assigning points to the nearest centroid.\n",
        "Updating centroids as the mean of assigned points.\n",
        "Repeating until convergence or max iterations.\n",
        "3. Explain the concept of a dendrogram in hierarchical clustering.\n",
        "\n",
        "A dendrogram is a tree-like diagram showing the hierarchical relationship between data points in hierarchical clustering. It visualizes the order and distance of merges (or splits) between clusters.\n",
        "\n",
        "4. What is the main difference between K-Means and Hierarchical Clustering?\n",
        "\n",
        "K-Means partitions data into a fixed number of clusters (K) using centroids, while hierarchical clustering builds a tree of clusters (dendrogram) without requiring a predefined K, allowing nested clusters.\n",
        "\n",
        "5. What are the advantages of DBSCAN over K-Means?\n",
        "\n",
        "Identifies clusters of arbitrary shape.\n",
        "Automatically detects noise/outliers.\n",
        "No need to specify the number of clusters.\n",
        "Robust to varying cluster densities.\n",
        "6. When would you use Silhouette Score in clustering?\n",
        "\n",
        "Silhouette Score is used to evaluate clustering quality by measuring how similar points are within their cluster compared to other clusters. It’s used to select optimal K or compare clustering algorithms.\n",
        "\n",
        "7. What are the limitations of Hierarchical Clustering?\n",
        "\n",
        "High computational complexity (O(n²) or O(n³)).\n",
        "Sensitive to noise and outliers.\n",
        "Hard to scale to large datasets.\n",
        "Fixed merges cannot be undone.\n",
        "8. Why is feature scaling important in clustering algorithms like K-Means?\n",
        "\n",
        "Feature scaling ensures all features contribute equally to distance calculations. K-Means relies on Euclidean distance, so unscaled features with larger ranges can dominate clustering.\n",
        "\n",
        "9. How does DBSCAN identify noise points?\n",
        "\n",
        "DBSCAN labels points as noise if they have fewer than min_samples neighbors within a radius eps and are not part of a dense cluster.\n",
        "\n",
        "10. Define inertia in the context of K-Means.\n",
        "\n",
        "Inertia is the sum of squared distances between each point and its assigned cluster centroid, measuring cluster compactness.\n",
        "\n",
        "11. What is the elbow method in K-Means clustering?\n",
        "\n",
        "The elbow method plots inertia against K (number of clusters) and selects K where adding more clusters yields diminishing reductions in inertia, forming an \"elbow\" shape.\n",
        "\n",
        "12. Describe the concept of \"density\" in DBSCAN.\n",
        "\n",
        "Density in DBSCAN refers to the number of points within a radius eps. Points with at least min_samples neighbors within eps are core points, forming dense regions (clusters).\n",
        "\n",
        "13. Can hierarchical clustering be used on categorical data?\n",
        "\n",
        "Yes, with appropriate distance metrics (e.g., Hamming distance) and linkage criteria. However, numerical data with Euclidean distance is more common.\n",
        "\n",
        "14. What does a negative Silhouette Score indicate?\n",
        "\n",
        "A negative Silhouette Score indicates that a point is closer to points in another cluster than its own, suggesting poor clustering quality or misassignment.\n",
        "\n",
        "15. Explain the term \"linkage criteria\" in hierarchical clustering.\n",
        "\n",
        "Linkage criteria define how distances between clusters are calculated in hierarchical clustering (e.g., single: minimum distance, complete: maximum distance, average: mean distance).\n",
        "\n",
        "16. Why might K-Means clustering perform poorly on data with varying cluster sizes or density?\n",
        "\n",
        "K-Means assumes spherical clusters of similar size and density. Varying sizes or densities lead to incorrect centroid placement and poor cluster separation.\n",
        "\n",
        "17. What are the core parameters in DBSCAN, and how do they influence clustering?\n",
        "\n",
        "eps: Radius for neighbor search; smaller values create tighter clusters, larger values merge clusters.\n",
        "min_samples: Minimum points to form a core point; higher values reduce noise but may miss small clusters.\n",
        "18. How does K-Means++ improve upon standard K-Means initialization?\n",
        "\n",
        "K-Means++ initializes centroids by choosing the first randomly, then selecting subsequent centroids with probability proportional to the distance from existing centroids, reducing poor initializations.\n",
        "\n",
        "19. What is agglomerative clustering?\n",
        "\n",
        "Agglomerative clustering is a bottom-up hierarchical clustering method that starts with each point as a cluster and iteratively merges the closest pairs based on a linkage criterion.\n",
        "\n",
        "20. What makes Silhouette Score a better metric than just inertia for model evaluation?\n",
        "\n",
        "Silhouette Score evaluates both cohesion (within-cluster distance) and separation (between-cluster distance), while inertia only measures within-cluster compactness, ignoring inter-cluster separation."
      ],
      "metadata": {
        "id": "_5cTYSz4NMSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"21. Generate synthetic data with 4 centers using make_blobs and apply K-Means clustering. Visualize using a scatter plot.\n",
        "python\"\"\"\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('K-Means Clustering (4 Centers)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fxVf8OJcP73q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22 Load the Iris dataset and use Agglomerative Clustering to group the data into 3 clusters. Display the first 10 predicted labels.\n",
        "\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=3)\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Print first 10 labels\n",
        "print(\"First 10 predicted labels:\", labels[:10])"
      ],
      "metadata": {
        "id": "jTlogJUCQGD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23 Generate synthetic data using make_moons and apply DBSCAN. Highlight outliers in the plot.\n",
        "\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Visualize (outliers are label -1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=np.where(labels == -1, 'red', labels), cmap='viridis')\n",
        "plt.title('DBSCAN Clustering (Outliers in Red)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E5eZZk3ZQL8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24 Load the Wine dataset and apply K-Means clustering after standardizing the features. Print the size of each cluster.\n",
        "\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Print cluster sizes\n",
        "print(\"Cluster sizes:\", np.bincount(labels))"
      ],
      "metadata": {
        "id": "3ztYr7tsQRYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#25 Use make_circles to generate synthetic data and cluster it using DBSCAN. Plot the result.\n",
        "\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_circles(n_samples=200, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('DBSCAN on Concentric Circles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RJA8nKKdQW-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26 Load the Breast Cancer dataset, apply MinMaxScaler, and use K-Means with 2 clusters. Output the cluster centroids.\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load data\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "# Scale\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Print centroids\n",
        "print(\"Cluster centroids:\\n\", kmeans.cluster_centers_)"
      ],
      "metadata": {
        "id": "kUDOJE01QcoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "27 Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\n",
        "\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=[0.5, 1.0, 1.5], random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0 perspectiva])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "TQVMc97vQj3T",
        "outputId": "4f3d6e0d-9d35-445b-ff9e-a5a47a7a96d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1-3135755819.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1-3135755819.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    27 Generate synthetic data using make_blobs with varying cluster standard deviations and cluster with DBSCAN.\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 28. Load the Digits dataset, reduce to 2D using PCA, and visualize clusters from K-Means.**\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce to 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('K-Means on Digits (PCA 2D)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iH3FvJG6Qt-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#29. Create synthetic data using make_blobs and evaluate silhouette scores for k=2 to 5. Display as a bar chart.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Compute silhouette scores\n",
        "scores = []\n",
        "for k in range(2, 6):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    labels = kmeans.fit_predict(X)\n",
        "    scores.append(silhouette_score(X, labels))\n",
        "\n",
        "# Visualize\n",
        "plt.bar(range(2, 6), scores)\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Scores for K-Means')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VGTTZiVMQ0Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#30 Load the Iris dataset and use hierarchical clustering to group data. Plot a dendrogram with average linkage.\n",
        "\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "Z = linkage(X, method='average')\n",
        "\n",
        "# Plot dendrogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "dendrogram(Z)\n",
        "plt.title('Dendrogram (Average Linkage) for Iris')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g14VnZjkQ8b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#31 Generate synthetic data with overlapping clusters using make_blobs, then apply K-Means and visualize with decision boundaries.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.5, random_state=42)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Create mesh grid for decision boundaries\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
        "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Visualize\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('K-Means with Decision Boundaries')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xvALo_hZRD5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Digits dataset and apply DBSCAN after reducing dimensions with t-SNE. Visualize the results.\n",
        "\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce to 2D\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=2, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_tsne)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('DBSCAN on Digits (t-SNE 2D)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t7fjl6gaRLRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data using make_blobs and apply Agglomerative Clustering with complete linkage. Plot the result.\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='complete')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering (Complete Linkage)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TaWSzOU-RTmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the Breast Cancer dataset and compare inertia values for K=2 to 6 using K-Means. Show results in a line plot.\n",
        "python\n",
        "\n",
        "Collapse\n",
        "\n",
        "Wrap\n",
        "\n",
        "Run\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "# Compute inertia\n",
        "inertias = []\n",
        "for k in range(2, 7):\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    kmeans.fit(X)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "# Visualize\n",
        "plt.plot(range(2, 7), inertias, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Inertia for K-Means on Breast Cancer')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6jB63BN5RZfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate synthetic concentric circles using make_circles and cluster using Agglomerative Clustering with single linkage.\n",
        "\n",
        "\n",
        "Copy\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_circles(n_samples=200, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=2, linkage='single')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering (Single Linkage) on Circles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oco_AvHeRdYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Use the Wine dataset, apply DBSCAN after scaling the data, and count the number of clusters (excluding noise).\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Count clusters (exclude noise: label -1)\n",
        "n_clusters = len(np.unique(labels)) - (1 if -1 in labels else 0)\n",
        "print(\"Number of clusters:\", n_clusters)"
      ],
      "metadata": {
        "id": "VsLeMyngRmHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate synthetic data with make_blobs and apply K-Means. Then plot the cluster centers on top of the data points.\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='x', s=200, linewidths=3)\n",
        "plt.title('K-Means with Cluster Centers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eikPqxtfRrUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset, cluster with DBSCAN, and print how many samples were identified as noise.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Count noise points (label -1)\n",
        "noise_count = np.sum(labels == -1)\n",
        "print(\"Number of noise points:\", noise_count)"
      ],
      "metadata": {
        "id": "fnNNbmdHRx2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate synthetic non-linearly separable data using make_moons, apply K-Means, and visualize the clustering result.\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('K-Means on Non-Linearly Separable Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D1d-HxhaR3OO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the Digits dataset, apply PCA to reduce to 3 components, then use K-Means and visualize with a 3D scatter plot.\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce to 3D\n",
        "pca = PCA(n_components=3)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "labels = kmeans.fit_predict(X_pca)\n",
        "\n",
        "# Visualize\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap='viridis')\n",
        "ax.set_title('K-Means on Digits (PCA 3D)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6-lRpRn0R8Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate synthetic blobs with 5 centers and apply K-Means. Then use silhouette_score to evaluate the clustering.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=500, centers=5, random_state=42)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Compute silhouette score\n",
        "score = silhouette_score(X, labels)\n",
        "print(\"Silhouette Score:\", score)"
      ],
      "metadata": {
        "id": "qaIVgxkySBX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Load the Breast Cancer dataset, reduce dimensionality using PCA, and apply Agglomerative Clustering. Visualize in 2D.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "\n",
        "# Reduce to 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=2)\n",
        "labels = agg.fit_predict(X_pca)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Breast Cancer (PCA 2D)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SlXzGGqHSFvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# Generate noisy circular data using make_circles and visualize clustering results from K-Means and DBSCAN side-by-side.\n",
        "\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_circles(n_samples=200, factor=0.5, noise=0.05, random_state=42)\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=2, random_state=42)\n",
        "kmeans_labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "dbscan_labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Visualize side-by-side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax1.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis')\n",
        "ax1.set_title('K-Means on Circles')\n",
        "ax2.scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='viridis')\n",
        "ax2.set_title('DBSCAN on Circles')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "635Vio-oSKoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset and plot the Silhouette Coefficient for each sample after K-Means clustering.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "# Compute silhouette scores\n",
        "silhouette_vals = silhouette_samples(X, labels)\n",
        "\n",
        "# Visualize\n",
        "plt.bar(range(len(silhouette_vals)), silhouette_vals)\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Silhouette Coefficient')\n",
        "plt.title('Silhouette Coefficients for Iris (K-Means)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M7PXaFUCSPdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#Generate synthetic data using make_blobs and apply Agglomerative Clustering with 'average' linkage. Visualize clusters.\n",
        "python\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=4, linkage='average')\n",
        "labels = agg.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering (Average Linkage)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jQgTDhUgSUvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Wine dataset, apply K-Means, and visualize the cluster assignments in a seaborn pairplot (first 4 features).\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "wine = load_wine()\n",
        "X = wine.data[:, :4]  # First 4 features\n",
        "df = pd.DataFrame(X, columns=wine.feature_names[:4])\n",
        "\n",
        "# Apply K-Means\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "df['Cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "# Visualize\n",
        "sns.pairplot(df, hue='Cluster', diag_kind='hist')\n",
        "plt.suptitle('K-Means Clustering on Wine (First 4 Features)', y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f3BtCn4NSjZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#6.9. Generate noisy blobs using make_blobs and use DBSCAN to identify both clusters and noise points. Print the count.\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Generate data\n",
        "X, _ = make_blobs(n_samples=300, centers=3, cluster_std=1.0, random_state=42)\n",
        "\n",
        "# Apply DBSCAN\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Count clusters and noise\n",
        "n_clusters = len(np.unique(labels)) - (1 if -1 in labels else 0)\n",
        "n_noise = np.sum(labels == -1)\n",
        "print(f\"Number of clusters: {n_clusters}, Number of noise points: {n_noise}\")"
      ],
      "metadata": {
        "id": "B-s1zupoSl9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the Digits dataset, reduce dimensions using t-SNE, then apply Agglomerative Clustering and plot the clusters.\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "\n",
        "# Reduce to 2D\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg = AgglomerativeClustering(n_clusters=10)\n",
        "labels = agg.fit_predict(X_tsne)\n",
        "\n",
        "# Visualize\n",
        "plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Agglomerative Clustering on Digits (t-SNE 2D)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uqQPvvNcNNAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}