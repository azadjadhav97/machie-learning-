{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIJHgWDMcmmz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a Decision Tree and how does it work?\n",
        "\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It works by recursively splitting the dataset into smaller subsets based on the features, creating a tree-like structure of decisions. Each internal node represents a test on a feature, each branch represents the outcome of the test, and each leaf node represents a class label (for classification) or a numerical value (for regression). The goal is to partition the data in such a way that the samples within each leaf node are as homogeneous as possible with respect to the target variable."
      ],
      "metadata": {
        "id": "BwWq3AJOdbtI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z7q9k6MWdcSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are impurity measures in Decision Trees?\n",
        "\n",
        "Impurity measures are metrics used to quantify the homogeneity of a set of data points within a node of a Decision Tree. They help determine the best split at each node by evaluating how well the split separates the data into distinct classes or values. Common impurity measures include Gini Impurity and Entropy."
      ],
      "metadata": {
        "id": "7TYM8XZHdlOM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wnfnEybKdmky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the mathematical formula for Gini Impurity?\n",
        "\n",
        "The Gini Impurity for a node t is calculated as: Gini(t) = 1 - Σ (p_i)² where p_i is the proportion of samples belonging to class i at node t. A Gini Impurity of 0 indicates perfect purity (all samples belong to the same class)."
      ],
      "metadata": {
        "id": "EPKtYSakdomQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lf1kigeAdqK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the mathematical formula for Entropy?\n",
        "\n",
        " The Entropy for a node t is calculated as: Entropy(t) = - Σ p_i * log₂(p_i) where p_i is the proportion of samples belonging to class i at node t. An Entropy of 0 indicates perfect purity. Entropy reaches its maximum value when the classes are equally distributed."
      ],
      "metadata": {
        "id": "VYsmuVx9dtPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        " Information Gain is the reduction in entropy or impurity achieved by splitting a node based on a particular feature. It measures how much information a feature provides about the target variable. In Decision Trees, the algorithm selects the feature that yields the highest Information Gain at each split, as this indicates the most effective way to reduce uncertainty and partition the data."
      ],
      "metadata": {
        "id": "GenNm0RIdwcI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCqVUICfd1W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        " Both Gini Impurity and Entropy are impurity measures used to evaluate the quality of splits in Decision Trees.\n",
        "\n",
        " Gini Impurity tends to be slightly faster to compute as it doesn't involve logarithms. It measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution.\n",
        "\n",
        "Entropy is based on information theory and measures the average amount of information needed to identify the class of a data point. It penalizes mixed classes more heavily than Gini Impurity. In practice, the choice between Gini Impurity and Entropy often has a minimal impact on the final tree structure and performance."
      ],
      "metadata": {
        "id": "2FRNNPggd2kj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jTDB6qIpd8x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the mathematical explanation behind Decision Trees?\n",
        "\n",
        "The mathematical explanation behind Decision Trees involves the recursive partitioning of the feature space. At each node, the algorithm selects a feature and a split point that minimizes the impurity of the resulting child nodes or maximizes the information gain. This process is repeated recursively until a stopping criterion is met (e.g., maximum depth, minimum samples per leaf). The splits define hyperplanes in the feature space, creating rectangular regions that correspond to the leaf nodes. For classification, the majority class in a leaf node is assigned as the prediction; for regression, the average value of the target variable in a leaf node is assigned."
      ],
      "metadata": {
        "id": "zQKV6efyeHMS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7rAne0keIdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-pruning is a technique used to prevent overfitting by stopping the growth of the Decision Tree during the training phase. This is done by setting constraints on the tree's structure, such as:\n",
        "\n",
        "Maximum depth of the tree\n",
        "Minimum number of samples required to split a node\n",
        "Minimum number of samples required in a leaf node\n",
        "Maximum number of leaf nodes"
      ],
      "metadata": {
        "id": "WXMvIkqteK2J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQVXvDRveMA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Post-Pruning in Decision Trees?\n",
        "\n",
        "Post-pruning is a technique used to reduce the complexity of a fully grown Decision Tree after it has been trained. It involves removing branches or nodes from the tree that do not significantly improve the model's performance on a validation set. This helps to simplify the tree and reduce the risk of overfitting to the training data. Common post-pruning methods include Reduced Error Pruning and Cost-Complexity Pruning."
      ],
      "metadata": {
        "id": "yQIsmRAzeQAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xm0zmXTDeROq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the difference between Pre-Pruning and Post-Pruning?\n",
        "\n",
        "The main difference lies in when the pruning is applied:\n",
        "\n",
        "Pre-pruning stops the tree growth early during training, preventing the creation of overly complex branches.\n",
        "Post-pruning prunes a fully grown tree after training, removing less informative branches. Pre-pruning is generally faster as it avoids building the full tree, but it can sometimes stop too early and miss potentially useful branches. Post-pruning can lead to more optimal trees but requires building and pruning the full tree, which can be computationally more expensive."
      ],
      "metadata": {
        "id": "Y096wDe0eUI-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FKICB2PieVYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is a Decision Tree Regressor?\n",
        "\n",
        "A Decision Tree Regressor is a Decision Tree algorithm specifically designed for regression tasks, where the goal is to predict a continuous numerical value. It works similarly to a Decision Tree Classifier but instead of assigning class labels to leaf nodes, it assigns a numerical value (typically the mean or median of the target variable for the samples in that leaf). The splits are made to minimize the variance or mean squared error within the resulting child nodes."
      ],
      "metadata": {
        "id": "sQaFYFsIeXKa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3c6xReReYM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the advantages and disadvantages of Decision Trees? Advantages:\n",
        "\n",
        "Easy to understand and interpret (white-box model)\n",
        "Can handle both numerical and categorical data\n",
        "Requires little data preparation (no need for feature scaling)\n",
        "Can model non-linear relationships\n",
        "Relatively fast to train and predict\n",
        "Disadvantages:\n",
        "\n",
        "Prone to overfitting, especially with complex trees\n",
        "Can be unstable; small changes in data can lead to large changes in the tree structure\n",
        "Can create biased trees if some classes dominate\n",
        "Optimal tree construction is an NP-complete problem; greedy algorithms are used in practice"
      ],
      "metadata": {
        "id": "7LxAwU5kebP0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wm4fRRnzecE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does a Decision Tree handle missing values?\n",
        "\n",
        " Decision Trees can handle missing values in several ways:\n",
        "\n",
        "Ignoring samples with missing values: This is the simplest approach but can lead to loss of data.\n",
        "Imputation: Missing values can be imputed using techniques like mean, median, or mode imputation before training.\n",
        "Surrogate splits: Some Decision Tree implementations can use surrogate splits, where if a sample has a missing value for the primary splitting feature, the algorithm uses an alternative feature that is highly correlated with the primary feature to make the split decision."
      ],
      "metadata": {
        "id": "eVO46nIPeeDj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DB0iFJ0yefwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How does a Decision Tree handle categorical features?\n",
        "\n",
        "Decision Trees can handle categorical features by splitting the data based on the different categories. For a binary split, the algorithm can group categories into two sets. For multi-way splits, it can create a separate branch for each category. In some implementations, categorical features are converted into numerical representations (e.g., one-hot encoding) before training"
      ],
      "metadata": {
        "id": "9lndDJWcei2U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbLoc2QaekAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are some real-world applications of Decision Trees?\n",
        "\n",
        " Decision Trees are used in various real-world applications, including:\n",
        "\n",
        "Medical Diagnosis: Classifying diseases based on symptoms and patient data.\n",
        "Credit Risk Assessment: Evaluating the creditworthiness of loan applicants.\n",
        "Marketing: Segmenting customers and predicting their behavior.\n",
        "Fraud Detection: Identifying fraudulent transactions.\n",
        "Spam Filtering: Classifying emails as spam or not spam.\n",
        "Image Classification: Classifying images based on their features."
      ],
      "metadata": {
        "id": "OyqsFIGgel66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 16\n",
        "# Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Decision Tree Classifier"
      ],
      "metadata": {
        "id": "-wHKVK3henGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 17\n",
        "# Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the\n",
        "# feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier using Gini Impurity\n",
        "dt_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_gini.fit(X_train, y_train)\n",
        "\n",
        "# Print the feature importances\n",
        "feature_importances = dt_gini.feature_importances_\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "print(\"Feature Importances (using Gini Impurity):\")\n",
        "for name, importance in zip(feature_names, feature_importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "id": "oYLkD0VeewJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 18\n",
        "# Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "# model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier using Entropy\n",
        "dt_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "dt_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_entropy = dt_entropy.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
        "print(f\"Accuracy of the Decision Tree Classifier (using Entropy) on the Iris dataset: {accuracy_entropy:.4f}\")"
      ],
      "metadata": {
        "id": "ViuirQ6ceyQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 19\n",
        "# Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "# Squared Error (MSE)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_regressor = dt_regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred_regressor)\n",
        "print(f\"Mean Squared Error (MSE) of the Decision Tree Regressor: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "yYMSCCIrez5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 20\n",
        "# Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "import graphviz\n",
        "from IPython.display import display\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=3) # Limiting depth for better visualization\n",
        "dt_classifier.fit(X, y)\n",
        "\n",
        "# Export the tree to a DOT format\n",
        "dot_data = export_graphviz(dt_classifier, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create a Graphviz object from the DOT data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Display the tree\n",
        "display(graph)"
      ],
      "metadata": {
        "id": "c4x-nQCNe158"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 21\n",
        "# Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "# accuracy with a fully grown tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree with max_depth=3\n",
        "dt_max_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_max_depth_3.fit(X_train, y_train)\n",
        "y_pred_depth_3 = dt_max_depth_3.predict(X_test)\n",
        "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
        "\n",
        "# Train a fully grown Decision Tree (default max_depth=None)\n",
        "dt_full = DecisionTreeClassifier(random_state=42)\n",
        "dt_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_depth_3:.4f}\")\n",
        "print(f\"Accuracy of fully grown Decision Tree: {accuracy_full:.4f}\")"
      ],
      "metadata": {
        "id": "n_jJ_yN9e32j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 22\n",
        "# Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "# accuracy with a default tree\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree with min_samples_split=5\n",
        "dt_min_split_5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "dt_min_split_5.fit(X_train, y_train)\n",
        "y_pred_min_split_5 = dt_min_split_5.predict(X_test)\n",
        "accuracy_min_split_5 = accuracy_score(y_test, y_pred_min_split_5)\n",
        "\n",
        "# Train a default Decision Tree (default min_samples_split=2)\n",
        "dt_default = DecisionTreeClassifier(random_state=42)\n",
        "dt_default.fit(X_train, y_train)\n",
        "y_pred_default = dt_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "\n",
        "print(f\"Accuracy of Decision Tree with min_samples_split=5: {accuracy_min_split_5:.4f}\")\n",
        "print(f\"Accuracy of default Decision Tree: {accuracy_default:.4f}\")"
      ],
      "metadata": {
        "id": "LITidtxhe56D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 23\n",
        "# Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "# accuracy with unscaled data\n",
        "\n",
        "# Note: Decision Trees are generally not sensitive to feature scaling,\n",
        "# but this code demonstrates how to apply it.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Train on unscaled data ---\n",
        "dt_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "dt_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = dt_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "\n",
        "# --- Apply feature scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Train on scaled data ---\n",
        "dt_scaled = DecisionTreeClassifier(random_state=42)\n",
        "dt_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = dt_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy of Decision Tree on unscaled data: {accuracy_unscaled:.4f}\")\n",
        "print(f\"Accuracy of Decision Tree on scaled data: {accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "id": "WjsiCHJ0e72c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 24\n",
        "# Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification.\n",
        "# Note: DecisionTreeClassifier in scikit-learn inherently handles multiclass classification\n",
        "# using a one-vs-one approach for the decision process at each node.\n",
        "# The OvR strategy is typically used with binary classifiers for multiclass problems.\n",
        "# However, we can demonstrate how to use OvR with a Decision Tree as the base estimator\n",
        "# using the OneVsRestClassifier wrapper.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "# Load the Iris dataset (it's a multiclass dataset)\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the base Decision Tree Classifier\n",
        "base_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize the One-vs-Rest Classifier with the Decision Tree as the base estimator\n",
        "ovr_classifier = OneVsRestClassifier(base_classifier)\n",
        "\n",
        "# Train the OvR Decision Tree Classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_ovr = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Accuracy of One-vs-Rest Decision Tree Classifier on the Iris dataset: {accuracy_ovr:.4f}\")"
      ],
      "metadata": {
        "id": "QqPZzpaxe9tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 25\n",
        "# Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = dt_classifier.feature_importances_\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Display feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "for name, importance in zip(feature_names, feature_importances):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "id": "UpSFEOoQfWl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 26\n",
        "# Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "# with an unrestricted tree\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor with max_depth=5\n",
        "dt_regressor_depth5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "dt_regressor_depth5.fit(X_train, y_train)\n",
        "y_pred_depth5 = dt_regressor_depth5.predict(X_test)\n",
        "mse_depth5 = mean_squared_error(y_test, y_pred_depth5)\n",
        "print(f\"Mean Squared Error (MSE) of Regressor with max_depth=5: {mse_depth5:.4f}\")\n",
        "\n",
        "# Train an unrestricted Decision Tree Regressor (default max_depth=None)\n",
        "dt_regressor_full = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor_full.fit(X_train, y_train)\n",
        "y_pred_full = dt_regressor_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "print(f\"Mean Squared Error (MSE) of Unrestricted Regressor: {mse_full:.4f}\")"
      ],
      "metadata": {
        "id": "JrlyJLKHfYXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 27\n",
        "# Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "# visualize its effect on accuracy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier and get the cost complexity pruning paths\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# Train Decision Trees with different alpha values\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Calculate accuracy for each tree\n",
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "\n",
        "# Visualize the effect of CCP on accuracy\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(ccp_alphas, train_scores, marker='o', label='train accuracy')\n",
        "plt.plot(ccp_alphas, test_scores, marker='o', label='test accuracy')\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('accuracy')\n",
        "plt.title('Accuracy vs alpha for training and test sets')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FVu5umUofaAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 28\n",
        "# Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "# Recall, and F1-Score\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "# Use average='weighted' for multiclass classification\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "rGYA0cB8fbrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 29\n",
        "# Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b8MLpB_OfdeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Programming Question 30\n",
        "# Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "# for max_depth and min_samples_split.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7, 10, None],\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Perform GridSearchCV to find the best parameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the best score\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_clf = grid_search.best_estimator_\n",
        "test_accuracy = best_clf.score(X_test, y_test)\n",
        "print(f\"Accuracy of the best model on the test set: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "XNKLTvaKffWl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}