{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Methods: Bagging and Random Forest\n",
    "This notebook provides a comprehensive overview of Bagging and Random Forest ensemble methods, covering both theoretical concepts and practical implementations using Python's scikit-learn library.\n",
    "\n",
    "## Table of Contents\n",
    "1.  Theoretical Explanations\n",
    "2.  Practical Examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Explanations of Ensemble Methods\n",
    "\n",
    "## 1. Can we use Bagging for regression problems?\n",
    "\n",
    "Yes, Bagging can absolutely be used for regression problems. The core principle of Bagging, which stands for Bootstrap Aggregating, involves training multiple base models on different bootstrap samples of the original dataset and then combining their predictions. For classification tasks, the predictions are typically combined through a majority vote (e.g., in a Bagging Classifier). For regression tasks, the predictions from the individual base regressors are averaged to produce the final output. This averaging helps to reduce variance and improve the overall stability and accuracy of the model, similar to how it works for classification. A common example of a Bagging Regressor is a Random Forest Regressor, which uses Decision Trees as its base estimators.\n",
    "\n",
    "## 2. What is the difference between multiple model training and single model training?\n",
    "\n",
    "**Single Model Training:**\n",
    "\n",
    "In single model training, a single machine learning model (e.g., a single Decision Tree, a single Logistic Regression model, or a single Support Vector Machine) is trained on the entire available dataset. The goal is to optimize the parameters of this single model to minimize errors and generalize well to unseen data. While simpler to implement and understand, single models can be prone to issues like overfitting (if the model is too complex for the data) or underfitting (if the model is too simple). Their performance is highly dependent on the specific algorithm chosen and the characteristics of the dataset.\n",
    "\n",
    "**Multiple Model Training (Ensemble Methods):**\n",
    "\n",
    "Multiple model training, often referred to as ensemble learning, involves training several individual models (called base learners or weak learners) and then combining their predictions to achieve a more robust and accurate overall prediction. The key idea is that by combining the strengths of multiple models, the weaknesses of individual models can be mitigated. Ensemble methods typically aim to reduce variance, bias, or both. There are various strategies for multiple model training, such as Bagging, Boosting, and Stacking, each with its own approach to training and combining models. The diversity among the base models is crucial for the success of ensemble methods.\n",
    "\n",
    "**Key Differences Summarized:**\n",
    "\n",
    "| Feature             | Single Model Training                               | Multiple Model Training (Ensemble Methods)                               |\n",
    "|---------------------|-----------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| **Number of Models**| One                                                 | Multiple                                                                 |\n",
    "| **Training Data**   | Entire dataset                                      | Varies (bootstrap samples, weighted samples, or different subsets)       |\n",
    "| **Prediction**      | Direct prediction from the single model             | Combined prediction (e.g., voting, averaging, weighted averaging)        |\n",
    "| **Goal**            | Optimize single model performance                   | Improve overall robustness, accuracy, and generalization by combining models |\n",
    "| **Complexity**      | Simpler to implement and interpret                  | More complex, but often yields better performance                        |\n",
    "| **Bias/Variance**   | Can be prone to high bias or high variance          | Aims to reduce bias or variance (or both)                                |\n",
    "| **Robustness**      | Less robust to noise or outliers                    | More robust due to diversified predictions                               |\n",
    "\n",
    "## 3. Explain the concept of feature randomness in Random Forest.\n",
    "\n",
    "Feature randomness, also known as feature bagging or subspace sampling, is a crucial concept in Random Forest that contributes significantly to its ability to reduce variance and prevent overfitting. In a standard Decision Tree, at each node, the algorithm considers all available features to find the best split. However, in a Random Forest, when building each individual Decision Tree, a random subset of features is selected at each split point. This means that instead of searching for the best feature among *all* features, the algorithm only considers a randomly chosen subset of `m` features (where `m` is typically much smaller than the total number of features `M`).\n",
    "\n",
    "Here's how it works and why it's beneficial:\n",
    "\n",
    "1.  **Random Subset Selection:** For each node in each tree, a new random subset of features is chosen. This subset is typically of a fixed size, often `sqrt(M)` for classification problems and `M/3` for regression problems, where `M` is the total number of features.\n",
    "2.  **Increased Diversity:** By restricting the features considered at each split, the individual trees in the forest become more diverse. If there are a few very strong predictor features, a standard Decision Tree would tend to pick these features at the top nodes of many trees, leading to highly correlated trees. Feature randomness forces the trees to explore other, potentially less obvious, features, thus decorrelating the trees.\n",
    "3.  **Reduced Variance:** The decorrelation of individual trees is key to reducing the overall variance of the Random Forest. When individual trees make errors, these errors are less likely to be correlated if the trees are diverse. Averaging (for regression) or majority voting (for classification) over these diverse and less correlated trees leads to a more stable and accurate ensemble prediction.\n",
    "4.  **Prevention of Overfitting:** By introducing randomness in feature selection, Random Forest prevents individual trees from overfitting to specific features or noise in the data. This makes the ensemble more robust and less prone to memorizing the training data.\n",
    "\n",
    "In essence, feature randomness, combined with bootstrap sampling (which creates diverse training datasets for each tree), ensures that the individual trees in a Random Forest are both strong (due to the tree-building process) and diverse (due to the randomness), leading to a powerful and generalized ensemble model.\n",
    "\n",
    "## 4. What is 'Out-of-Bag' (OOB) score?\n",
    "\n",
    "The Out-of-Bag (OOB) score is a powerful and convenient way to estimate the generalization error of a Bagging-based ensemble model, such as a Random Forest, without the need for a separate validation set or cross-validation. It leverages the inherent nature of bootstrap sampling.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1.  **Bootstrap Sampling:** In Bagging, each base estimator (e.g., a Decision Tree in a Random Forest) is trained on a bootstrap sample of the original training data. A bootstrap sample is created by randomly sampling with replacement from the original dataset. This means that some data points will appear multiple times in a bootstrap sample, while others will not appear at all.\n",
    "2.  **Out-of-Bag Samples:** For each base estimator, the data points that were *not* included in its bootstrap sample are called \n",
    "\n",
    "\n",
    "the \"out-of-bag\" (OOB) samples for that specific estimator. On average, about 36.8% of the original data points will be OOB for any given bootstrap sample.\n",
    "3.  **OOB Prediction:** For each data point in the original dataset, we can identify the base estimators for which this data point was an OOB sample. We then use these specific estimators to make a prediction for that data point. For example, if a data point `x` was OOB for trees T1, T5, and T10, then only T1, T5, and T10 would be used to predict `x`.\n",
    "4.  **Aggregating OOB Predictions:** Once predictions are made for all OOB samples by their respective OOB estimators, these predictions are aggregated. For classification, this typically involves a majority vote among the OOB predictions. For regression, it involves averaging the OOB predictions.\n",
    "5.  **OOB Score Calculation:** The OOB score is then calculated by comparing these aggregated OOB predictions to the true labels (or values) of the original data points. For classification, it's usually the accuracy score (proportion of correctly classified OOB samples). For regression, it could be the Mean Squared Error or R-squared score.\n",
    "\n",
    "**Advantages of OOB Score:**\n",
    "\n",
    "*   **No Need for Separate Validation Set:** The OOB score provides an unbiased estimate of the generalization error without requiring a separate validation set, which means more data can be used for training the model.\n",
    "*   **Computational Efficiency:** It's computed during the training process, adding minimal computational overhead.\n",
    "*   **Robustness:** It's a robust estimate of performance, especially for large datasets.\n",
    "\n",
    "In essence, the OOB score acts as an internal cross-validation mechanism within the Bagging process, providing a reliable measure of how well the ensemble model is likely to perform on unseen data.\n",
    "\n",
    "## 5. How can you measure the importance of features in a Random Forest model?\n",
    "\n",
    "Feature importance in a Random Forest model can be measured in a couple of primary ways, both of which leverage the ensemble nature of the algorithm:\n",
    "\n",
    "### a) Mean Decrease in Impurity (MDI) / Gini Importance\n",
    "\n",
    "This is the most common and default method for calculating feature importance in tree-based models like Random Forest. It's based on how much each feature contributes to reducing the impurity (e.g., Gini impurity for classification, Mean Squared Error for regression) across all the Decision Trees in the forest.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1.  **Impurity Reduction:** When a Decision Tree makes a split at a node, it does so by selecting the feature and split point that best reduces the impurity of the data. The amount of impurity reduction achieved by a split is a measure of its importance.\n",
    "2.  **Averaging Across Trees:** For each feature, the impurity reduction it provides is summed up over all nodes where that feature is used for splitting, across all individual trees in the Random Forest. This sum is then averaged across all trees.\n",
    "3.  **Normalization:** The final importance scores are typically normalized so that their sum is 1.\n",
    "\n",
    "**Pros:**\n",
    "*   Easy to compute and readily available in most implementations (e.g., `feature_importances_` attribute in scikit-learn).\n",
    "*   Provides a quick understanding of which features are generally more influential in the model's predictions.\n",
    "\n",
    "**Cons:**\n",
    "*   **Bias towards high-cardinality features:** Features with many unique values (e.g., continuous features) or categorical features with many categories can appear more important because they offer more splitting possibilities, even if they are not truly more predictive.\n",
    "*   **Correlated features:** If two features are highly correlated, the Random Forest might arbitrarily pick one over the other for splitting, leading to a diluted importance score for both, even if they are both important.\n",
    "\n",
    "### b) Permutation Importance (Mean Decrease in Accuracy / Performance)\n",
    "\n",
    "Permutation importance is a more robust and less biased method for feature importance, especially when dealing with correlated features or features with varying cardinalities. It measures the decrease in a model's performance (e.g., accuracy for classification, MSE for regression) when a single feature's values are randomly shuffled (permuted).\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1.  **Train the Model:** First, a Random Forest model is trained on the original dataset.\n",
    "2.  **Record Baseline Performance:** The model's performance (e.g., accuracy) is evaluated on a validation set (or OOB samples).\n",
    "3.  **Permute a Feature:** For each feature, its values are randomly shuffled (permuted) in the validation set, while all other features remain unchanged.\n",
    "4.  **Re-evaluate Performance:** The model's performance is re-evaluated on this permuted dataset.\n",
    "5.  **Calculate Importance:** The decrease in performance (baseline performance - permuted performance) indicates the importance of that feature. A large drop in performance suggests that the feature was important for the model's predictions.\n",
    "6.  **Repeat and Average:** This process is repeated multiple times (e.g., 5-10 times) for each feature, and the results are averaged to get a more stable importance score.\n",
    "\n",
    "**Pros:**\n",
    "*   **Less biased:** Not biased towards high-cardinality features.\n",
    "*   **Handles correlated features better:** If two features are correlated, permuting one will still show its importance, as the other correlated feature might not be able to fully compensate for its absence.\n",
    "*   **Model-agnostic:** Can be applied to any trained model, not just tree-based models.\n",
    "*   **Directly related to model performance:** The importance score directly reflects how much the model's performance degrades when the feature's information is removed.\n",
    "\n",
    "**Cons:**\n",
    "*   More computationally expensive than MDI, as it requires multiple re-evaluations of the model.\n",
    "\n",
    "While MDI is often sufficient for a quick glance at feature importance, permutation importance is generally preferred for more reliable and interpretable results, especially in critical applications.\n",
    "\n",
    "## 6. Explain the working principle of a Bagging Classifier.\n",
    "\n",
    "A Bagging Classifier, short for Bootstrap Aggregating Classifier, is an ensemble meta-algorithm that combines the predictions from multiple base classifiers to produce a more robust and accurate classification. Its working principle can be broken down into three main steps:\n",
    "\n",
    "1.  **Bootstrap Sampling (Resampling with Replacement):**\n",
    "    *   From the original training dataset of size `N`, `M` new training subsets (bootstrap samples) are created. Each bootstrap sample is generated by randomly sampling `N` data points from the original dataset *with replacement*. This means that some data points from the original dataset may appear multiple times in a single bootstrap sample, while others may not appear at all. On average, each bootstrap sample will contain about 63.2% of the unique original data points, with the remaining 36.8% being the \n",
    "\n",
    "\n",
    "out-of-bag (OOB) samples.\n",
    "    *   The purpose of bootstrap sampling is to create diverse training sets for each base classifier. This diversity is crucial because it ensures that the individual classifiers are not identical, leading to a more robust ensemble.\n",
    "\n",
    "2.  **Base Classifier Training:**\n",
    "    *   A separate base classifier (also known as a weak learner) is trained independently on each of the `M` bootstrap samples. These base classifiers are typically of the same type (e.g., Decision Trees, Logistic Regression, Support Vector Machines). Decision Trees are a popular choice because they are sensitive to changes in the training data, which makes them good candidates for Bagging.\n",
    "    *   Each base classifier learns its own patterns and decision boundaries from its specific bootstrap sample.\n",
    "\n",
    "3.  **Aggregation of Predictions (Majority Voting):**\n",
    "    *   Once all `M` base classifiers are trained, when a new, unseen data point needs to be classified, each base classifier makes its own prediction.\n",
    "    *   For classification tasks, the final prediction of the Bagging Classifier is determined by a majority vote among the predictions of all the base classifiers. For example, if 7 out of 10 base classifiers predict 'Class A' and 3 predict 'Class B', the Bagging Classifier will predict 'Class A'.\n",
    "\n",
    "**Key Benefits of Bagging:**\n",
    "\n",
    "*   **Variance Reduction:** The primary benefit of Bagging is its ability to reduce variance. By averaging (or majority voting) the predictions of multiple models trained on different subsets of the data, the impact of noise or specific characteristics of any single training set is diminished. This makes the ensemble model less sensitive to fluctuations in the training data and more stable.\n",
    "*   **Overfitting Reduction:** By reducing variance, Bagging effectively helps in reducing overfitting, especially with complex base models like deep Decision Trees. Each tree might overfit to its specific bootstrap sample, but the aggregation process smooths out these individual overfitting tendencies.\n",
    "*   **Increased Robustness:** The ensemble becomes more robust to outliers and noisy data because the errors of individual models tend to cancel each other out.\n",
    "\n",
    "In summary, a Bagging Classifier builds multiple diverse base classifiers by training them on different bootstrap samples of the data and then combines their predictions through majority voting to achieve a more accurate and generalized classification.\n",
    "\n",
    "## 7. How do you evaluate a Bagging Classifier's performance?\n",
    "\n",
    "Evaluating a Bagging Classifier's performance involves using standard classification metrics, similar to how you would evaluate any other classification model. However, due to the ensemble nature and the use of bootstrap sampling, there are specific considerations and additional metrics that can be particularly useful.\n",
    "\n",
    "Here are the common ways to evaluate a Bagging Classifier's performance:\n",
    "\n",
    "1.  **Hold-out Validation (Train-Test Split):**\n",
    "    *   The most straightforward method is to split your original dataset into a training set and a separate, unseen test set (e.g., 70% training, 30% testing). The Bagging Classifier is trained on the training set, and its performance is then evaluated on the test set.\n",
    "    *   **Metrics:** Common metrics include:\n",
    "        *   **Accuracy:** The proportion of correctly classified instances. (Number of correct predictions / Total number of predictions).\n",
    "        *   **Precision:** The proportion of true positive predictions among all positive predictions. (True Positives / (True Positives + False Positives)). Useful when the cost of False Positives is high.\n",
    "        *   **Recall (Sensitivity):** The proportion of true positive predictions among all actual positive instances. (True Positives / (True Positives + False Negatives)). Useful when the cost of False Negatives is high.\n",
    "        *   **F1-Score:** The harmonic mean of Precision and Recall. It provides a single score that balances both precision and recall. (2 * (Precision * Recall) / (Precision + Recall)).\n",
    "        *   **Confusion Matrix:** A table that summarizes the performance of a classification model on a set of test data, showing the number of true positives, true negatives, false positives, and false negatives.\n",
    "        *   **ROC Curve and AUC Score:** The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (Recall) against the False Positive Rate at various threshold settings. The Area Under the Curve (AUC) provides an aggregate measure of performance across all possible classification thresholds. A higher AUC indicates better performance.\n",
    "\n",
    "2.  **Cross-Validation (e.g., K-Fold Cross-Validation):**\n",
    "    *   For a more robust estimate of performance, especially with smaller datasets, K-Fold Cross-Validation is highly recommended. The dataset is divided into K folds. The model is trained on K-1 folds and validated on the remaining fold. This process is repeated K times, with each fold serving as the validation set once. The average performance across all K folds is then reported.\n",
    "    *   **Benefits:** Provides a more reliable estimate of the model's generalization ability by reducing the dependence on a single train-test split.\n",
    "\n",
    "3.  **Out-of-Bag (OOB) Score:**\n",
    "    *   As discussed in Question 4, the OOB score is a unique and highly valuable evaluation metric for Bagging classifiers. It provides an internal, unbiased estimate of the generalization error without the need for a separate validation set.\n",
    "    *   The OOB score (e.g., OOB accuracy for classification) can be directly used as a performance indicator during training.\n",
    "\n",
    "**Practical Considerations:**\n",
    "\n",
    "*   **Class Imbalance:** If your dataset has imbalanced classes, accuracy alone might be misleading. In such cases, Precision, Recall, F1-Score, and ROC-AUC are more informative.\n",
    "*   **Computational Cost:** Evaluating a Bagging Classifier can be more computationally intensive than a single model due to the multiple base estimators. However, the OOB score offers an efficient alternative.\n",
    "*   **Ensemble vs. Base Model:** It's often useful to compare the performance of the Bagging Classifier against a single base estimator (e.g., a single Decision Tree) to demonstrate the benefits of ensembling.\n",
    "\n",
    "By using a combination of these evaluation techniques, you can gain a comprehensive understanding of your Bagging Classifier's performance and its ability to generalize to new data.\n",
    "\n",
    "## 8. How does a Bagging Regressor work?\n",
    "\n",
    "A Bagging Regressor operates on the same fundamental principles as a Bagging Classifier, but it is adapted for regression tasks, where the goal is to predict a continuous numerical output rather than a discrete class label. The core steps remain similar:\n",
    "\n",
    "1.  **Bootstrap Sampling:**\n",
    "    *   Just like with classification, the Bagging Regressor starts by creating multiple (e.g., `M`) bootstrap samples from the original training dataset. Each bootstrap sample is created by randomly sampling data points *with replacement* from the original dataset. This process ensures that each base regressor is trained on a slightly different subset of the data, promoting diversity among the individual models.\n",
    "\n",
    "2.  **Base Regressor Training:**\n",
    "    *   A separate base regressor (e.g., a Decision Tree Regressor, a Linear Regression model, or a K-Nearest Neighbors Regressor) is trained independently on each of the `M` bootstrap samples. Decision Tree Regressors are a common choice due to their ability to capture complex non-linear relationships and their sensitivity to data variations.\n",
    "    *   Each base regressor learns to map input features to continuous output values based on its specific bootstrap sample.\n",
    "\n",
    "3.  **Aggregation of Predictions (Averaging):**\n",
    "    *   Once all `M` base regressors are trained, when a new, unseen data point needs a prediction, each base regressor makes its own continuous numerical prediction.\n",
    "    *   The final prediction of the Bagging Regressor is then determined by *averaging* the predictions of all the individual base regressors. This averaging process is crucial for regression tasks, as it smooths out the individual predictions and reduces the overall variance of the ensemble.\n",
    "\n",
    "**Key Benefits of Bagging for Regression:**\n",
    "\n",
    "*   **Variance Reduction:** The primary advantage of Bagging for regression is its effectiveness in reducing the variance of the model. Individual base regressors, especially complex ones like deep Decision Trees, can be prone to overfitting and high variance. By averaging their predictions, the Bagging Regressor mitigates these individual fluctuations, leading to a more stable and robust prediction.\n",
    "*   **Improved Accuracy:** By reducing variance, Bagging often leads to improved prediction accuracy compared to a single base regressor.\n",
    "*   **Robustness to Noise:** The averaging process makes the ensemble less sensitive to noisy data or outliers in the training set.\n",
    "\n",
    "**Example:** If you have 100 Decision Tree Regressors in your Bagging ensemble, and for a new data point, they predict values like 10.2, 9.8, 10.5, 9.9, etc., the Bagging Regressor would output the average of these values, say 10.1. This averaged prediction is generally more reliable than any single tree's prediction.\n",
    "\n",
    "In essence, a Bagging Regressor leverages the power of multiple diverse base regressors, trained on bootstrap samples, and combines their predictions through averaging to produce a more accurate, stable, and robust continuous output.\n",
    "\n",
    "## 9. What is the main advantage of ensemble techniques?\n",
    "\n",
    "The main advantage of ensemble techniques is their ability to **significantly improve model performance (accuracy, stability, and robustness) compared to single, individual models.** This improvement stems from the principle of \n",
    "combining multiple \"weak learners\" or diverse models to create a more powerful \"strong learner.\" Here's a breakdown of how this advantage manifests:\n",
    "\n",
    "1.  **Reduced Variance (Bagging):** Ensemble methods like Bagging (e.g., Random Forest) are particularly effective at reducing variance. Individual models, especially complex ones like deep Decision Trees, can be prone to overfitting the training data, leading to high variance. By training multiple models on different subsets of the data (bootstrap samples) and averaging or voting their predictions, the ensemble smooths out the individual models' tendencies to overfit to specific noise or patterns in their respective training subsets. This results in a more stable and generalized model.\n",
    "\n",
    "2.  **Reduced Bias (Boosting):** Ensemble methods like Boosting (e.g., AdaBoost, Gradient Boosting) primarily focus on reducing bias. They sequentially build models, with each new model attempting to correct the errors made by the previous ones. By iteratively focusing on misclassified or poorly predicted instances, Boosting algorithms can learn complex relationships in the data that a single model might miss, thereby reducing systematic errors (bias).\n",
    "\n",
    "3.  **Improved Accuracy:** By addressing both variance and bias (or one more predominantly than the other, depending on the ensemble type), ensemble methods generally achieve higher predictive accuracy than their individual base learners. The collective intelligence of multiple models often outperforms any single model.\n",
    "\n",
    "4.  **Increased Robustness:** Ensemble models are more robust to noisy data, outliers, and the specific characteristics of a single training set. Errors made by individual models tend to cancel each other out when their predictions are combined, leading to a more reliable overall prediction.\n",
    "\n",
    "5.  **Better Generalization:** The combination of reduced variance and bias leads to better generalization performance on unseen data. Ensemble models are less likely to overfit or underfit, making them more reliable in real-world applications.\n",
    "\n",
    "6.  **Feature Importance (Random Forest):** Specific ensemble methods like Random Forest provide a natural way to estimate feature importance, which can be invaluable for understanding the underlying data and for feature selection.\n",
    "\n",
    "In essence, ensemble techniques leverage the diversity and collective wisdom of multiple models to overcome the limitations of individual models, leading to superior predictive performance and greater reliability.\n",
    "\n",
    "## 10. What is the main challenge of ensemble methods?\n",
    "\n",
    "While ensemble methods offer significant advantages in terms of performance, they also come with certain challenges. The main challenge of ensemble methods is **increased complexity and reduced interpretability** compared to single models.\n",
    "\n",
    "Here's a breakdown of this challenge:\n",
    "\n",
    "1.  **Reduced Interpretability (Black Box Nature):**\n",
    "    *   **Difficulty in Understanding Decisions:** A single Decision Tree, for example, is relatively easy to interpret: you can trace the path from the root to a leaf node to understand why a particular prediction was made. In contrast, an ensemble model, especially one with hundreds or thousands of base estimators (like a Random Forest or Gradient Boosting Machine), makes decisions based on the aggregation of many individual models. It becomes extremely difficult, if not impossible, to trace the exact reasoning behind a single prediction.\n",
    "    *   **Lack of Transparency:** This \n",
    "lack of transparency can be a significant drawback in applications where understanding *why* a prediction was made is as important as the prediction itself (e.g., in medical diagnosis, loan approvals, or legal contexts). If your application requires clear, transparent, and explainable decisions (e.g., in medical diagnosis, legal judgments, or financial credit scoring where regulatory compliance demands explainability), then simpler, more interpretable models (like a single Decision Tree, Logistic Regression, or Linear Regression) might be preferred, even if they offer slightly lower accuracy.\n",
    "\n",
    "2.  **Increased Computational Cost:**\n",
    "    *   **Training Time:** Training multiple models, especially if they are complex or if the dataset is large, can be significantly more time-consuming and computationally expensive than training a single model.\n",
    "    *   **Prediction Time:** Making predictions with an ensemble also takes longer, as each base model needs to make a prediction, and then these predictions need to be aggregated.\n",
    "    *   **Memory Usage:** Storing multiple models can require more memory than storing a single model.\n",
    "\n",
    "3.  **Hyperparameter Tuning Complexity:**\n",
    "    *   Ensemble methods often introduce additional hyperparameters related to the ensemble itself (e.g., number of estimators, learning rate, subsample ratio) in addition to the hyperparameters of the base estimators. Tuning these can be more complex and time-consuming.\n",
    "\n",
    "4.  **Potential for Diminishing Returns:**\n",
    "    *   While ensembles generally improve performance, there can be a point of diminishing returns. Adding more base estimators beyond a certain number might not lead to significant performance gains but will definitely increase computational cost.\n",
    "\n",
    "5.  **Overfitting (in some cases, e.g., Boosting):**\n",
    "    *   While Bagging helps reduce overfitting, some ensemble methods, particularly Boosting algorithms, can still overfit if not properly regularized or if trained for too many iterations. They can become too specialized to the training data, losing their ability to generalize.\n",
    "\n",
    "In summary, the trade-off for the superior performance of ensemble methods is often a sacrifice in interpretability and an increase in computational resources required for training and prediction.\n",
    "\n",
    "## 11. Explain the key idea behind ensemble techniques.\n",
    "\n",
    "The key idea behind ensemble techniques in machine learning is to **combine the predictions of multiple individual models (often called base learners or weak learners) to achieve a more accurate, robust, and generalized prediction than any single model could achieve on its own.** It's analogous to the wisdom of crowds, where the collective judgment of a diverse group often outperforms the judgment of any single expert.\n",
    "\n",
    "Here are the core principles that underpin this idea:\n",
    "\n",
    "1.  **Diversity:** The most crucial aspect of successful ensemble methods is the diversity among the base models. If all individual models make the same errors or learn the same patterns, combining them won't lead to much improvement. Diversity can be introduced in several ways:\n",
    "    *   **Different Training Data:** Training models on different subsets of the data (e.g., bootstrap samples in Bagging).\n",
    "    *   **Different Features:** Training models on different subsets of features (e.g., feature randomness in Random Forest).\n",
    "    *   **Different Algorithms:** Using different types of base learning algorithms (e.g., combining Decision Trees, SVMs, and Logistic Regression in Stacking).\n",
    "    *   **Different Hyperparameters:** Using the same algorithm but with different hyperparameter settings.\n",
    "\n",
    "2.  **Error Compensation:** When individual models are diverse, their errors are likely to be uncorrelated or partially correlated. If one model makes a mistake on a particular instance, another diverse model might correctly classify or predict that instance. By aggregating their predictions (e.g., through voting for classification or averaging for regression), these individual errors tend to cancel each other out, leading to a more accurate overall prediction.\n",
    "\n",
    "3.  **Bias-Variance Trade-off Management:** Ensemble methods are powerful tools for managing the bias-variance trade-off:\n",
    "    *   **Reducing Variance:** Techniques like Bagging primarily focus on reducing variance. By averaging predictions from multiple models trained on slightly different data, the ensemble becomes less sensitive to the specific noise or characteristics of any single training set, thus reducing overfitting.\n",
    "    *   **Reducing Bias:** Techniques like Boosting primarily focus on reducing bias. They sequentially build models that learn from the mistakes of previous models, iteratively improving the model's ability to capture complex patterns and reduce systematic errors.\n",
    "\n",
    "4.  **Improved Generalization:** The ultimate goal is to build a model that generalizes well to unseen data. By combining diverse models and effectively managing the bias-variance trade-off, ensemble methods tend to produce models that are more robust and perform better on new, unseen examples.\n",
    "\n",
    "In essence, ensemble techniques leverage the idea that \n",
    "a collection of models, each with its own strengths and weaknesses, can collectively make better decisions than any single model working in isolation.\n",
    "\n",
    "## 12. What is a Random Forest Classifier?\n",
    "\n",
    "A Random Forest Classifier is a powerful and widely used ensemble machine learning algorithm that belongs to the Bagging family. It is essentially an ensemble of many individual Decision Tree Classifiers. The \n",
    "\"randomness\" in Random Forest comes from two main sources:\n",
    "\n",
    "1.  **Bootstrap Aggregating (Bagging):** Each individual Decision Tree in the Random Forest is trained on a different bootstrap sample of the original training data. A bootstrap sample is created by sampling with replacement from the original dataset. This introduces diversity among the trees, as each tree sees a slightly different subset of the data.\n",
    "\n",
    "2.  **Feature Randomness (Feature Bagging):** At each node of each Decision Tree, instead of considering all available features for the best split, the algorithm randomly selects a subset of features. The best split is then found only among these randomly selected features. This further decorrelates the trees, making them more independent and less prone to overfitting.\n",
    "\n",
    "**Working Principle:**\n",
    "\n",
    "1.  **Create Multiple Bootstrap Samples:** From the original training dataset, `N` (typically hundreds or thousands) bootstrap samples are created.\n",
    "2.  **Train Individual Decision Trees:** For each bootstrap sample, a Decision Tree is trained. During the tree-building process, at each split, only a random subset of features is considered.\n",
    "3.  **Aggregate Predictions (Majority Vote):** When a new data point needs to be classified, each of the `N` Decision Trees in the forest makes its own prediction. The final prediction of the Random Forest Classifier is determined by a majority vote among the predictions of all the individual trees. For example, if 70 out of 100 trees predict \"Class A\" and 30 predict \"Class B\", the Random Forest will classify the data point as \"Class A\".\n",
    "\n",
    "**Key Advantages of Random Forest Classifier:**\n",
    "\n",
    "*   **High Accuracy:** Often achieves very high accuracy due to the combination of multiple diverse trees.\n",
    "*   **Reduced Overfitting:** The randomness introduced through bootstrap sampling and feature randomness significantly reduces the risk of overfitting, making it a robust model.\n",
    "*   **Handles High-Dimensional Data:** Performs well even with a large number of features.\n",
    "*   **Handles Missing Values:** Can handle missing values implicitly or explicitly.\n",
    "*   **Feature Importance:** Provides a reliable measure of feature importance, indicating which features are most influential in the classification.\n",
    "*   **Robust to Outliers:** Less sensitive to outliers in the data.\n",
    "\n",
    "**Comparison to a Single Decision Tree:**\n",
    "\n",
    "A single Decision Tree can be prone to overfitting, especially if it's allowed to grow deep. It can capture noise in the training data and may not generalize well to unseen data. A Random Forest, by combining many such trees and introducing randomness, overcomes these limitations, leading to a more stable, accurate, and generalized model.\n",
    "\n",
    "## 13. What are the main types of ensemble techniques?\n",
    "\n",
    "Ensemble techniques can be broadly categorized into several main types, each with a distinct approach to combining models and addressing different aspects of model error (bias or variance). The three most prominent types are:\n",
    "\n",
    "1.  **Bagging (Bootstrap Aggregating):**\n",
    "    *   **Idea:** Reduces variance by training multiple models on different bootstrap samples (random sampling with replacement) of the original dataset and then averaging their predictions (for regression) or taking a majority vote (for classification).\n",
    "    *   **Diversity:** Achieved by training on different subsets of the data.\n",
    "    *   **Goal:** Primarily aims to reduce variance and prevent overfitting, especially with complex base models.\n",
    "    *   **Examples:** Bagging Classifier, Bagging Regressor, Random Forest (a specialized form of Bagging that also introduces feature randomness).\n",
    "\n",
    "2.  **Boosting:**\n",
    "    *   **Idea:** Reduces bias by sequentially building models, where each new model attempts to correct the errors made by the previous ones. Models are trained iteratively, with each iteration focusing more on the instances that were misclassified or poorly predicted by the preceding models.\n",
    "    *   **Diversity:** Achieved by weighting the training data (giving more importance to misclassified instances) or by fitting residuals.\n",
    "    *   **Goal:** Primarily aims to reduce bias and convert weak learners into strong learners.\n",
    "    *   **Examples:** AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "3.  **Stacking (Stacked Generalization):**\n",
    "    *   **Idea:** Combines predictions from multiple diverse models (base models or level-0 models) using another machine learning model (meta-model or level-1 model). The predictions of the base models serve as input features for the meta-model.\n",
    "    *   **Diversity:** Achieved by using different types of base learning algorithms (e.g., Decision Tree, SVM, Logistic Regression) and/or different subsets of data.\n",
    "    *   **Goal:** Aims to leverage the strengths of various algorithms and often achieves higher accuracy than individual models or simpler ensembles.\n",
    "    *   **Process:**\n",
    "        *   **Level 0 (Base Models):** Train several diverse base models on the training data.\n",
    "        *   **Level 1 (Meta-Model):** Use the predictions of the base models as new features to train a meta-model. This meta-model learns how to best combine the predictions of the base models.\n",
    "    *   **Examples:** A Stacking Classifier or Regressor combining various algorithms.\n",
    "\n",
    "**Other less common or specialized types include:**\n",
    "\n",
    "*   **Voting/Averaging:** A simpler form of ensemble where multiple models (often trained independently) simply vote (for classification) or average (for regression) their predictions. This is a very basic form of ensemble that doesn't involve complex training strategies like Bagging or Boosting.\n",
    "*   **Mixture of Experts:** A modular approach where different \n",
    "models (experts) specialize in different regions of the input space, and a gating network decides which expert to use for a given input.\n",
    "\n",
    "Each type of ensemble technique has its strengths and weaknesses, and the choice depends on the specific problem, dataset characteristics, and desired trade-offs between accuracy, interpretability, and computational cost.\n",
    "\n",
    "## 14. What is ensemble learning in machine learning?\n",
    "\n",
    "Ensemble learning is a general machine learning paradigm where the goal is to **improve model performance (accuracy, stability, and robustness) by combining the predictions from multiple individual models, rather than relying on a single model.** The fundamental idea is that a group of \"weak learners\" or diverse models can collectively form a \"strong learner\" that outperforms any single constituent model.\n",
    "\n",
    "Think of it like a committee of experts: instead of trusting a single expert, you consult several experts, each with their own perspective and knowledge, and then aggregate their opinions to arrive at a more informed and reliable decision. In machine learning, these \"experts\" are individual models, and their \"opinions\" are their predictions.\n",
    "\n",
    "**Core Principles of Ensemble Learning:**\n",
    "\n",
    "1.  **Diversity:** For an ensemble to be effective, the individual models within it must be diverse. If all models make the same errors, combining them won't help. Diversity can be achieved through:\n",
    "    *   **Data Perturbation:** Training models on different subsets of the training data (e.g., bootstrap samples in Bagging).\n",
    "    *   **Feature Perturbation:** Training models on different subsets of features (e.g., feature randomness in Random Forest).\n",
    "    *   **Algorithm Diversity:** Using different types of learning algorithms (e.g., combining Decision Trees, SVMs, Logistic Regression).\n",
    "    *   **Hyperparameter Diversity:** Using the same algorithm but with different hyperparameter settings.\n",
    "\n",
    "2.  **Aggregation:** Once individual models make their predictions, these predictions need to be combined. The aggregation method depends on the task:\n",
    "    *   **Classification:** Majority voting (each model gets one vote, the class with most votes wins), weighted voting.\n",
    "    *   **Regression:** Averaging (simple or weighted average of predictions).\n",
    "    *   **More Complex Aggregation:** Training another model (meta-learner) to combine the predictions (as in Stacking).\n",
    "\n",
    "**Why Ensemble Learning Works (Addressing Bias-Variance Trade-off):**\n",
    "\n",
    "Ensemble methods are effective because they can address different aspects of the bias-variance trade-off:\n",
    "\n",
    "*   **Reducing Variance (Bagging):** By averaging predictions from multiple models trained on different data subsets, ensembles can reduce the impact of noise and specific characteristics of any single training set. This makes the overall model more stable and less prone to overfitting.\n",
    "*   **Reducing Bias (Boosting):** By sequentially building models that focus on correcting errors made by previous models, ensembles can learn more complex patterns and reduce systematic errors, leading to a better fit to the underlying data distribution.\n",
    "\n",
    "**Benefits of Ensemble Learning:**\n",
    "\n",
    "*   **Improved Accuracy:** Often leads to higher predictive accuracy than single models.\n",
    "*   **Increased Robustness:** More resilient to noise, outliers, and data fluctuations.\n",
    "*   **Better Generalization:** Less prone to overfitting or underfitting, resulting in better performance on unseen data.\n",
    "*   **Enhanced Stability:** Predictions are more stable and less sensitive to small changes in the training data.\n",
    "\n",
    "Ensemble learning has become a cornerstone of modern machine learning, widely used in various applications due to its consistent ability to deliver superior performance.\n",
    "\n",
    "## 15. When should we avoid using ensemble methods?\n",
    "\n",
    "While ensemble methods offer significant advantages, there are certain situations or considerations where their use might be less ideal or even detrimental:\n",
    "\n",
    "1.  **When Interpretability is Paramount:**\n",
    "    *   **Challenge:** Ensemble models, especially complex ones like Random Forests or Gradient Boosting Machines with many trees, are often considered \n",
    "black boxes. It's difficult to understand the exact reasoning behind a specific prediction, as it's an aggregation of many individual models. If your application requires clear, transparent, and explainable decisions (e.g., in medical diagnosis, legal judgments, or financial credit scoring where regulatory compliance demands explainability), then simpler, more interpretable models (like a single Decision Tree, Logistic Regression, or Linear Regression) might be preferred, even if they offer slightly lower accuracy.\n",
    "\n",
    "2.  **When Computational Resources are Severely Limited:**\n",
    "    *   **Challenge:** Ensemble methods involve training and storing multiple models, which can be computationally expensive in terms of both training time, prediction time, and memory usage. If you are working with extremely large datasets, real-time prediction requirements with very low latency, or resource-constrained environments (e.g., embedded systems, mobile devices with limited processing power and memory), the overhead of ensemble methods might be prohibitive. In such cases, a simpler, faster, and less resource-intensive single model might be a better choice.\n",
    "\n",
    "3.  **When the Base Models are Already Very Strong and Diverse:**\n",
    "    *   **Challenge:** If your individual base models are already performing exceptionally well and are highly diverse (i.e., they make different types of errors), then the additional gains from ensembling might be marginal. While ensembles almost always provide some improvement, the effort and complexity might not be justified for a very small performance boost.\n",
    "\n",
    "4.  **When the Data is Very Small or Simple:**\n",
    "    *   **Challenge:** For very small datasets, the benefits of bootstrap sampling (in Bagging) might be limited, and the ensemble might not be able to learn diverse patterns effectively. For very simple, linearly separable data, a simple linear model might achieve near-perfect accuracy with much less complexity than an ensemble.\n",
    "\n",
    "5.  **When Overfitting is a Concern with Boosting (and not properly regularized):**\n",
    "    *   **Challenge:** While Bagging helps reduce overfitting, Boosting algorithms, if not carefully tuned and regularized, can be prone to overfitting, especially if trained for too many iterations. If you lack the expertise or time to properly tune Boosting models, they can sometimes perform worse than simpler models.\n",
    "\n",
    "6.  **When Data Leakage is a Risk:**\n",
    "    *   **Challenge:** While not exclusive to ensembles, if data leakage is present in your features, ensemble methods might amplify the problem by leveraging these spurious correlations more effectively, leading to overly optimistic performance on training data but poor generalization.\n",
    "\n",
    "In summary, while ensemble methods are powerful, they are not a one-size-fits-all solution. The decision to use them should consider the trade-offs between performance gains, interpretability requirements, computational resources, and the characteristics of the dataset and problem at hand.\n",
    "\n",
    "## 16. How does Bagging help in reducing overfitting?\n",
    "\n",
    "Bagging (Bootstrap Aggregating) is highly effective in reducing overfitting, particularly when using complex and high-variance base models like deep Decision Trees. It achieves this reduction through two primary mechanisms:\n",
    "\n",
    "1.  **Variance Reduction through Averaging/Voting:**\n",
    "    *   **Overfitting and High Variance:** Overfitting occurs when a model learns the training data too well, including its noise and specific patterns, leading to poor generalization on unseen data. This is often associated with high variance, meaning the model's predictions are highly sensitive to small changes in the training data.\n",
    "    *   **Diverse Training Sets:** Bagging creates multiple training sets (bootstrap samples) by sampling with replacement from the original dataset. Each base model is trained on a slightly different subset of the data. This means that each base model will likely overfit to different aspects of its specific training subset, capturing different noise patterns or local optima.\n",
    "    *   **Smoothing Effect:** When the predictions from these individually overfitted (but diverse) base models are aggregated (averaged for regression, majority voted for classification), the noise and specific overfitting tendencies of individual models tend to cancel each other out. The averaging/voting process smooths out the high variance of individual models, leading to a more stable and generalized overall prediction. It's like taking multiple noisy measurements and averaging them to get a more accurate reading.\n",
    "\n",
    "2.  **Decorrelation of Base Models:**\n",
    "    *   **Correlated Errors:** If all base models were trained on the exact same data, they would likely make similar errors and overfit in similar ways. Combining them would not yield much benefit.\n",
    "    *   **Bootstrap Sampling for Diversity:** Bootstrap sampling naturally introduces diversity among the base models. Because each base model sees a slightly different training set, they learn different decision boundaries and patterns. This leads to their errors being less correlated.\n",
    "    *   **Random Forest Enhancement:** In Random Forest, an additional layer of randomness (feature randomness) is introduced, further decorrelating the trees. By considering only a random subset of features at each split, the trees are forced to explore different predictive paths, making them even more independent. This enhanced decorrelation significantly boosts the overfitting reduction capability.\n",
    "\n",
    "**Analogy:** Imagine you have a group of students, each studying a slightly different textbook on the same subject. Each student might memorize different specific details (overfit to their textbook). However, if you ask all of them a question and average their answers, the collective answer is likely to be more accurate and less prone to the specific memorization errors of any single student. Bagging works similarly by leveraging the collective intelligence of diverse, slightly overfitted models to produce a robust and generalized ensemble.\n",
    "\n",
    "In essence, Bagging reduces overfitting by creating an ensemble of diverse base models, each trained on a slightly different view of the data, and then averaging or voting their predictions to smooth out individual model variances and cancel out their specific overfitting tendencies.\n",
    "\n",
    "## 17. Why is Random Forest better than a single Decision Tree?\n",
    "\n",
    "Random Forest is generally considered superior to a single Decision Tree for several compelling reasons, primarily stemming from its ensemble nature and the introduction of randomness:\n",
    "\n",
    "1.  **Reduced Overfitting (Primary Reason):**\n",
    "    *   **Single Decision Tree:** A single Decision Tree, especially when allowed to grow deep (unpruned), can easily overfit the training data. It can learn the noise and specific patterns in the training set too well, leading to perfect performance on training data but poor generalization on unseen data. This is because it creates very specific, often complex, decision boundaries that perfectly separate the training examples.\n",
    "    *   **Random Forest:** Random Forest mitigates overfitting through two main mechanisms:\n",
    "        *   **Bagging (Bootstrap Aggregating):** Each tree is trained on a different bootstrap sample of the data. This means each tree sees a slightly different version of the training data and will overfit to different aspects of it. When their predictions are averaged (or voted), these individual overfitting tendencies cancel each other out, leading to a more stable and generalized model.\n",
    "        *   **Feature Randomness:** At each split in each tree, only a random subset of features is considered. This further decorrelates the trees, preventing them from all relying on the same strong predictors and forcing them to explore alternative features. This diversity makes the ensemble more robust.\n",
    "\n",
    "2.  **Higher Accuracy:**\n",
    "    *   By reducing overfitting and combining the predictions of many diverse trees, Random Forest typically achieves significantly higher predictive accuracy than a single Decision Tree. The collective wisdom of the ensemble generally outperforms any individual tree.\n",
    "\n",
    "3.  **Improved Generalization:**\n",
    "    *   Because it is less prone to overfitting, Random Forest generalizes much better to unseen data. It builds a more robust and stable model that captures the underlying patterns rather than memorizing the training examples.\n",
    "\n",
    "4.  **Robustness to Noise and Outliers:**\n",
    "    *   The ensemble nature makes Random Forest more robust to noisy data and outliers. Individual trees might be affected by noise, but their impact is diluted when averaged across many trees.\n",
    "\n",
    "5.  **Handles High-Dimensional Data Well:**\n",
    "    *   Random Forest can effectively handle datasets with a large number of features, even when many of them are irrelevant. The feature randomness helps in selecting relevant features across different trees.\n",
    "\n",
    "6.  **Implicit Feature Importance:**\n",
    "    *   Random Forest provides a convenient way to estimate feature importance, indicating which features are most influential in the model's predictions. This is a valuable insight for feature selection and understanding the data.\n",
    "\n",
    "**When a Single Decision Tree Might Be Preferred:**\n",
    "\n",
    "While Random Forest is generally superior, a single Decision Tree might be preferred in very specific scenarios:\n",
    "\n",
    "*   **Interpretability is paramount:** If you absolutely need to understand every single decision rule and trace the exact path of a prediction, a simple, shallow Decision Tree is much more interpretable than a Random Forest.\n",
    "*   **Very small datasets:** For extremely small datasets, the benefits of ensembling might be limited, and the overhead might not be justified.\n",
    "*   **Computational constraints:** If computational resources (time, memory) are extremely limited, a single Decision Tree is faster to train and predict with.\n",
    "\n",
    "However, for most practical machine learning tasks, Random Forest offers a significant performance advantage over a single Decision Tree due to its ability to effectively manage the bias-variance trade-off and reduce overfitting.\n",
    "\n",
    "## 18. What is the role of bootstrap sampling in Bagging?\n",
    "\n",
    "Bootstrap sampling is the cornerstone of Bagging (Bootstrap Aggregating) and plays a crucial role in its effectiveness. It is a resampling technique where multiple subsets of the original training data are created by randomly sampling with replacement. Here's its role:\n",
    "\n",
    "1.  **Creating Diverse Training Sets:**\n",
    "    *   **Mechanism:** From an original dataset of size `N`, `M` new datasets (bootstrap samples) are generated. Each bootstrap sample is also of size `N` and is created by drawing `N` data points randomly from the original dataset *with replacement*. This means that some original data points will appear multiple times in a bootstrap sample, while others may not appear at all.\n",
    "    *   **Purpose:** This process ensures that each base model in the ensemble is trained on a slightly different version of the training data. This slight variation in the training sets leads to diversity among the individual base models. If all models were trained on the exact same data, they would likely be identical or highly correlated, and combining them would offer little benefit.\n",
    "\n",
    "2.  **Reducing Variance and Overfitting:**\n",
    "    *   **Individual Model Overfitting:** Base models, especially complex ones like deep Decision Trees, are prone to overfitting to the specific noise and patterns present in their training data. When trained on different bootstrap samples, each base model will overfit in slightly different ways.\n",
    "    *   **Smoothing Effect:** When the predictions from these diverse, individually overfitted models are aggregated (averaged for regression, majority voted for classification), the random errors and specific overfitting tendencies of individual models tend to cancel each other out. This smoothing effect significantly reduces the overall variance of the ensemble model, making it more stable and less prone to overfitting to the training data.\n",
    "\n",
    "3.  **Enabling Out-of-Bag (OOB) Evaluation:**\n",
    "    *   **OOB Samples:** A direct consequence of bootstrap sampling is the existence of \n",
    "out-of-bag (OOB) samples. For each base model, approximately 36.8% of the original data points are not included in its bootstrap sample. These OOB samples can be used as a built-in validation set for that specific base model.\n",
    "    *   **Internal Validation:** By using the OOB samples, we can estimate the generalization error of the entire Bagging ensemble without the need for a separate validation set or cross-validation. This is a highly efficient and unbiased way to evaluate the model's performance during training.\n",
    "\n",
    "In summary, bootstrap sampling is fundamental to Bagging because it creates the necessary diversity among the base models, which in turn allows the ensemble to reduce variance, mitigate overfitting, and provide an internal mechanism for performance evaluation.\n",
    "\n",
    "## 19. What are some real-world applications of ensemble techniques?\n",
    "\n",
    "Ensemble techniques are widely used across various industries and domains due to their superior performance and robustness. Here are some prominent real-world applications:\n",
    "\n",
    "1.  **Healthcare and Medicine:**\n",
    "    *   **Disease Diagnosis:** Predicting the presence or absence of diseases (e.g., cancer detection from medical images, heart disease prediction) by combining predictions from multiple models trained on patient data, lab results, and imaging.\n",
    "    *   **Drug Discovery:** Identifying potential drug candidates or predicting drug efficacy and toxicity.\n",
    "    *   **Personalized Medicine:** Tailoring treatments based on individual patient characteristics by leveraging ensemble models to analyze complex genomic and clinical data.\n",
    "\n",
    "2.  **Finance and Banking:**\n",
    "    *   **Fraud Detection:** Identifying fraudulent transactions (credit card fraud, insurance fraud) by combining models that analyze transaction patterns, user behavior, and historical data. Ensemble methods are highly effective here due to their ability to handle imbalanced datasets and detect subtle anomalies.\n",
    "    *   **Credit Scoring:** Assessing the creditworthiness of loan applicants.\n",
    "    *   **Algorithmic Trading:** Predicting stock prices or market trends to inform trading strategies.\n",
    "    *   **Risk Management:** Quantifying and managing various financial risks.\n",
    "\n",
    "3.  **E-commerce and Retail:**\n",
    "    *   **Recommendation Systems:** Suggesting products to customers based on their past purchases, browsing history, and similar user behavior. Ensembles can combine collaborative filtering, content-based filtering, and other models.\n",
    "    *   **Customer Churn Prediction:** Identifying customers likely to stop using a service or product.\n",
    "    *   **Sales Forecasting:** Predicting future sales based on historical data, promotions, and external factors.\n",
    "    *   **Personalized Marketing:** Targeting marketing campaigns to specific customer segments.\n",
    "\n",
    "4.  **Image and Speech Recognition:**\n",
    "    *   **Object Detection and Classification:** In computer vision, ensembles of deep learning models (e.g., convolutional neural networks) are often used to improve accuracy in tasks like facial recognition, autonomous driving, and medical image analysis.\n",
    "    *   **Speech Recognition:** Enhancing the accuracy of converting spoken language to text.\n",
    "\n",
    "5.  **Natural Language Processing (NLP):**\n",
    "    *   **Sentiment Analysis:** Determining the emotional tone of text (positive, negative, neutral) in customer reviews, social media posts, etc.\n",
    "    *   **Spam Detection:** Classifying emails as spam or not spam.\n",
    "    *   **Text Classification:** Categorizing documents or articles into predefined topics.\n",
    "\n",
    "6.  **Cybersecurity:**\n",
    "    *   **Intrusion Detection Systems:** Identifying malicious network activity or cyber threats.\n",
    "    *   **Malware Detection:** Classifying software as malicious or benign.\n",
    "\n",
    "7.  **Manufacturing and Quality Control:**\n",
    "    *   **Predictive Maintenance:** Forecasting equipment failures to schedule maintenance proactively.\n",
    "    *   **Defect Detection:** Identifying defects in manufactured products.\n",
    "\n",
    "8.  **Environmental Science and Meteorology:**\n",
    "    *   **Weather Forecasting:** Combining predictions from multiple atmospheric models to improve accuracy.\n",
    "    *   **Climate Modeling:** Predicting long-term climate patterns.\n",
    "\n",
    "These examples highlight the versatility and effectiveness of ensemble techniques in solving complex real-world problems where high accuracy, robustness, and generalization are critical.\n",
    "\n",
    "## 20. What is the difference between Bagging and Boosting?\n",
    "\n",
    "Bagging and Boosting are two of the most popular and effective ensemble learning techniques, but they differ fundamentally in their approach to building and combining models. Both aim to improve predictive performance, but they do so by addressing different aspects of model error (variance vs. bias).\n",
    "\n",
    "Here's a detailed comparison:\n",
    "\n",
    "| Feature             | Bagging (Bootstrap Aggregating)                               | Boosting                                                              |\n",
    "|---------------------|---------------------------------------------------------------|-----------------------------------------------------------------------|\n",
    "| **Core Idea**       | Reduces variance by averaging/voting predictions from multiple models trained on different data subsets. | Reduces bias by sequentially building models, where each new model corrects errors of previous ones. |\n",
    "| **Model Training**  | **Parallel:** Base models are trained independently and in parallel. | **Sequential:** Base models are trained one after another, with each subsequent model learning from the mistakes of its predecessors. |\n",
    "| **Data Sampling**   | Uses **bootstrap sampling** (sampling with replacement) to create diverse training subsets for each base model. | Typically uses **weighted sampling** or focuses on **residuals/errors**. Instances that were misclassified or poorly predicted by previous models are given higher weight or attention in subsequent iterations. |\n",
    "| **Base Models**     | Often uses **strong learners** (e.g., deep Decision Trees) that are prone to overfitting and high variance. | Often uses **weak learners** (e.g., shallow Decision Trees, stumps) that are slightly better than random guessing. |\n",
    "| **Weighting**       | Each base model has an **equal weight** in the final prediction. | Base models are often **weighted** based on their performance, and data points are weighted based on how difficult they are to classify/predict. |\n",
    "| **Error Focus**     | Primarily aims to reduce **variance** and prevent overfitting. | Primarily aims to reduce **bias** and convert weak learners into strong learners. |\n",
    "| **Final Prediction**| **Averaging** (for regression) or **Majority Voting** (for classification) of individual model predictions. | **Weighted sum** of individual model predictions. |\n",
    "| **Robustness to Outliers/Noise** | Generally **more robust** to noise and outliers because individual errors tend to cancel out. | Can be **sensitive to outliers** and noisy data, as they might be given high weights in subsequent iterations, leading to overfitting to noise. |\n",
    "| **Computational Cost** | Can be computationally intensive due to training multiple models, but parallelizable. | Can be computationally intensive due to sequential nature, but often more efficient in terms of number of models needed for good performance. |\n",
    "| **Overfitting Risk**| **Less prone to overfitting** (especially Random Forest) due to variance reduction. | **Can overfit** if not properly regularized or if trained for too many iterations. |\n",
    "| **Examples**        | Bagging Classifier, Bagging Regressor, **Random Forest**. | AdaBoost, Gradient Boosting Machines (GBM), **XGBoost, LightGBM, CatBoost**. |\n",
    "\n",
    "**Analogy:**\n",
    "\n",
    "*   **Bagging:** Imagine a committee where each member independently studies the problem from a slightly different perspective (different data samples) and then they all vote on the final decision. No one member tries to correct another; they just contribute their independent opinion.\n",
    "*   **Boosting:** Imagine a team where members work sequentially. The first member tries to solve the problem. The second member then focuses specifically on the mistakes made by the first, and so on. Each subsequent member learns from and tries to improve upon the previous members' performance.\n",
    "\n",
    "In summary, Bagging builds diverse models in parallel to reduce variance, while Boosting builds models sequentially to reduce bias by focusing on previous errors. Both are powerful, but their strengths lie in addressing different types of model errors.\n",
    "\n",
    "## 21. Theoretical Question: What is ensemble learning in machine learning?\n",
    "\n",
    "This question was already answered as question 14. Please refer to section 14 for the explanation of ensemble learning.\n",
    "\n",
    "## 22. Theoretical Question: When should we avoid using ensemble methods?\n",
    "\n",
    "This question was already answered as question 15. Please refer to section 15 for the explanation of when to avoid using ensemble methods.\n",
    "\n",
    "## 23. Theoretical Question: How does Bagging help in reducing overfitting?\n",
    "\n",
    "This question was already answered as question 16. Please refer to section 16 for the explanation of how Bagging helps in reducing overfitting.\n",
    "\n",
    "## 24. Theoretical Question: Why is Random Forest better than a single Decision Tree?\n",
    "\n",
    "This question was already answered as question 17. Please refer to section 17 for the explanation of why Random Forest is better than a single Decision Tree.\n",
    "\n",
    "## 25. Theoretical Question: What is the role of bootstrap sampling in Bagging?\n",
    "\n",
    "This question was already answered as question 18. Please refer to section 18 for the explanation of the role of bootstrap sampling in Bagging.\n",
    "\n",
    "## 26. Theoretical Question: What are some real-world applications of ensemble techniques?\n",
    "\n",
    "This question was already answered as question 19. Please refer to section 19 for the explanation of real-world applications of ensemble techniques.\n",
    "\n",
    "## 27. Theoretical Question: What is the difference between Bagging and Boosting?\n",
    "\n",
    "This question was already answered as question 20. Please refer to section 20 for the explanation of the difference between Bagging and Boosting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Examples of Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Bagging Classifier with Decision Tree as the base estimator\n",
    "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Bagging Classifier\n",
    "bag_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Bagging Regressor with Decision Tree as the base estimator\n",
    "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Bagging Regressor\n",
    "bag_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = bag_reg.predict(X_test)\n",
    "\n",
    "# Calculate and print the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Bagging Regressor MSE: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = rf_clf.feature_importances_\n",
    "\n",
    "# Create a pandas Series for better visualization\n",
    "importance_df = pd.Series(feature_importances, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "print(\"Feature Importances (Random Forest Classifier):\")\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a Random Forest Regressor and compare its performance with a single Decision Tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "y_pred_rf = rf_reg.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
    "\n",
    "# Initialize and train a single Decision Tree Regressor\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train, y_train)\n",
    "y_pred_dt = dt_reg.predict(X_test)\n",
    "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
    "print(f\"Single Decision Tree Regressor MSE: {mse_dt:.4f}\")\n",
    "\n",
    "if mse_rf < mse_dt:\n",
    "    print(\"Random Forest Regressor performs better than a single Decision Tree Regressor.\")\n",
    "else:\n",
    "    print(\"Single Decision Tree Regressor performs better or equal to Random Forest Regressor.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compute the Out-of-Bag (OOB) score for a Random Forest Classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets (OOB score is calculated on training data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest Classifier with oob_score=True\n",
    "rf_clf_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rf_clf_oob.fit(X_train, y_train)\n",
    "\n",
    "# Get the OOB score\n",
    "oob_score = rf_clf_oob.oob_score_\n",
    "print(f\"Random Forest Classifier OOB Score: {oob_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train a Bagging Classifier using SVM as a base estimator and print accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Bagging Classifier with SVC as the base estimator\n",
    "# Using a linear kernel for faster training on a sample dataset\n",
    "bag_clf_svm = BaggingClassifier(estimator=SVC(kernel=\'linear\', probability=True, random_state=42), n_estimators=10, random_state=42)\n",
    "\n",
    "# Train the Bagging Classifier\n",
    "bag_clf_svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_svm = bag_clf_svm.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Bagging Classifier with SVM Accuracy: {accuracy_svm:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train a Random Forest Classifier with different numbers of trees and compare accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define different numbers of trees to test\n",
    "n_estimators_list = [10, 50, 100, 200, 500]\n",
    "\n",
    "print(\"Comparing Random Forest Classifier Accuracy with different numbers of trees:\")\n",
    "for n_estimators in n_estimators_list:\n",
    "    rf_clf_n = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    rf_clf_n.fit(X_train, y_train)\n",
    "    y_pred_n = rf_clf_n.predict(X_test)\n",
    "    accuracy_n = accuracy_score(y_test, y_pred_n)\n",
    "    print(f\"  n_estimators={n_estimators}: Accuracy = {accuracy_n:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Bagging Classifier with Logistic Regression as the base estimator\n",
    "bag_clf_lr = BaggingClassifier(estimator=LogisticRegression(random_state=42, solver=\'liblinear\'), n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Bagging Classifier\n",
    "bag_clf_lr.fit(X_train, y_train)\n",
    "\n",
    "# Get probability predictions for the positive class\n",
    "y_prob_lr = bag_clf_lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate and print the AUC score\n",
    "auc_score = roc_auc_score(y_test, y_prob_lr)\n",
    "print(f\"Bagging Classifier with Logistic Regression AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train a Random Forest Regressor and analyze feature importance scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Generate a sample dataset with feature names\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize a Random Forest Regressor\n",
    "rf_reg_imp = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Random Forest Regressor\n",
    "rf_reg_imp.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances_reg = rf_reg_imp.feature_importances_\n",
    "\n",
    "# Create a pandas Series for better visualization\n",
    "importance_df_reg = pd.Series(feature_importances_reg, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "print(\"Feature Importances (Random Forest Regressor):\")\n",
    "print(importance_df_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Bagging Classifier\n",
    "bag_clf_comp = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bag_clf_comp.fit(X_train, y_train)\n",
    "y_pred_bag = bag_clf_comp.predict(X_test)\n",
    "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_bag:.4f}\")\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "rf_clf_comp = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_comp.fit(X_train, y_train)\n",
    "y_pred_rf_comp = rf_clf_comp.predict(X_test)\n",
    "accuracy_rf_comp = accuracy_score(y_test, y_pred_rf_comp)\n",
    "print(f\"Random Forest Classifier Accuracy: {accuracy_rf_comp:.4f}\")\n",
    "\n",
    "if accuracy_rf_comp > accuracy_bag:\n",
    "    print(\"Random Forest Classifier performed better than Bagging Classifier.\")\n",
    "elif accuracy_bag > accuracy_rf_comp:\n",
    "    print(\"Bagging Classifier performed better than Random Forest Classifier.\")\n",
    "else:\n",
    "    print(\"Both Bagging and Random Forest Classifiers performed equally.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    \'n_estimators\': [50, 100, 200],\n",
    "    \'max_depth\': [None, 10, 20],\n",
    "    \'min_samples_split\': [2, 5]\n",
    "}\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_clf_grid = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf_clf_grid, param_grid=param_grid, cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found by GridSearchCV:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set with the best estimator\n",
    "best_rf_clf = grid_search.best_estimator_\n",
    "test_accuracy = best_rf_clf.score(X_test, y_test)\n",
    "print(f\"Test set accuracy with best parameters: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Train a Bagging Regressor with different numbers of base estimators and compare performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define different numbers of base estimators to test\n",
    "n_estimators_list_reg = [10, 50, 100, 200, 500]\n",
    "\n",
    "print(\"Comparing Bagging Regressor MSE with different numbers of base estimators:\")\n",
    "for n_estimators in n_estimators_list_reg:\n",
    "    bag_reg_n = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n",
    "    bag_reg_n.fit(X_train, y_train)\n",
    "    y_pred_n_reg = bag_reg_n.predict(X_test)\n",
    "    mse_n_reg = mean_squared_error(y_test, y_pred_n_reg)\n",
    "    print(f\"  n_estimators={n_estimators}: MSE = {mse_n_reg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Train a Random Forest Classifier and analyze misclassified samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest Classifier\n",
    "rf_clf_mis = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_mis.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_mis = rf_clf_mis.predict(X_test)\n",
    "\n",
    "# Identify misclassified samples\n",
    "misclassified_indices = np.where(y_test != y_pred_mis)[0]\n",
    "\n",
    "print(f\"Total test samples: {len(y_test)}\")\n",
    "print(f\"Number of misclassified samples: {len(misclassified_indices)}\")\n",
    "print(\"Indices of misclassified samples:\", misclassified_indices)\n",
    "\n",
    "# Optionally, print some details about the first few misclassified samples\n",
    "if len(misclassified_indices) > 0:\n",
    "    print(\"\\nDetails of first 5 misclassified samples (if available):\")\n",
    "    for i, idx in enumerate(misclassified_indices[:5]):\n",
    "        print(f\"  Sample Index: {idx}, True Label: {y_test[idx]}, Predicted Label: {y_pred_mis[idx]}\")\n",
    "        # print(f\"  Features: {X_test[idx]}\") # Uncomment to see features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Bagging Classifier\n",
    "bag_clf_comp_dt = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bag_clf_comp_dt.fit(X_train, y_train)\n",
    "y_pred_bag_dt = bag_clf_comp_dt.predict(X_test)\n",
    "accuracy_bag_dt = accuracy_score(y_test, y_pred_bag_dt)\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_bag_dt:.4f}\")\n",
    "\n",
    "# Train a single Decision Tree Classifier\n",
    "single_dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "single_dt_clf.fit(X_train, y_train)\n",
    "y_pred_single_dt = single_dt_clf.predict(X_test)\n",
    "accuracy_single_dt = accuracy_score(y_test, y_pred_single_dt)\n",
    "print(f\"Single Decision Tree Classifier Accuracy: {accuracy_single_dt:.4f}\")\n",
    "\n",
    "if accuracy_bag_dt > accuracy_single_dt:\n",
    "    print(\"Bagging Classifier performed better than a single Decision Tree Classifier.\")\n",
    "elif accuracy_single_dt > accuracy_bag_dt:\n",
    "    print(\"Single Decision Tree Classifier performed better or equal to Bagging Classifier.\")\n",
    "else:\n",
    "    print(\"Both Bagging and Single Decision Tree Classifiers performed equally.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Train a Random Forest Classifier and visualize the confusion matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest Classifier\n",
    "rf_clf_cm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_cm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_cm = rf_clf_cm.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_cm)\n",
    "\n",
    "# Display the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=rf_clf_cm.classes_)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix for Random Forest Classifier\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define base estimators\n",
    "estimators = [\n",
    "    (\"dt\", DecisionTreeClassifier(random_state=42)),\n",
    "    (\"svm\", SVC(kernel=\"linear\", probability=True, random_state=42)),\n",
    "    (\"lr\", LogisticRegression(solver=\"liblinear\", random_state=42))\n",
    "]\n",
    "\n",
    "# Initialize Stacking Classifier with a final estimator (e.g., Logistic Regression)\n",
    "st_clf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(solver=\"liblinear\", random_state=42), cv=5)\n",
    "\n",
    "# Train the Stacking Classifier\n",
    "st_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_st = st_clf.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy_st = accuracy_score(y_test, y_pred_st)\n",
    "print(f\"Stacking Classifier Accuracy: {accuracy_st:.4f}\")\n",
    "\n",
    "# Optionally, compare with individual base estimator accuracies\n",
    "print(\"\\nIndividual Base Estimator Accuracies:\")\n",
    "for name, estimator in estimators:\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred_base = estimator.predict(X_test)\n",
    "    acc_base = accuracy_score(y_test, y_pred_base)\n",
    "    print(f\"  {name.upper()} Accuracy: {acc_base:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Train a Random Forest Classifier and print the top 5 most important features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Generate a sample dataset with feature names\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest Classifier\n",
    "rf_clf_top5 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_top5.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances_top5 = rf_clf_top5.feature_importances_\n",
    "\n",
    "# Create a pandas Series for better visualization and sort\n",
    "importance_df_top5 = pd.Series(feature_importances_top5, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 5 Most Important Features (Random Forest Classifier):\")\n",
    "print(importance_df_top5.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Bagging Classifier\n",
    "bag_clf_metrics = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "bag_clf_metrics.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_metrics = bag_clf_metrics.predict(X_test)\n",
    "\n",
    "# Calculate and print precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred_metrics)\n",
    "recall = recall_score(y_test, y_pred_metrics)\n",
    "f1 = f1_score(y_test, y_pred_metrics)\n",
    "\n",
    "print(f\"Bagging Classifier Precision: {precision:.4f}\")\n",
    "print(f\"Bagging Classifier Recall: {recall:.4f}\")\n",
    "print(f\"Bagging Classifier F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define different max_depth values to test\n",
    "max_depth_list = [None, 5, 10, 15, 20]\n",
    "\n",
    "print(\"Comparing Random Forest Classifier Accuracy with different max_depth values:\")\n",
    "for depth in max_depth_list:\n",
    "    rf_clf_depth = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
    "    rf_clf_depth.fit(X_train, y_train)\n",
    "    y_pred_depth = rf_clf_depth.predict(X_test)\n",
    "    accuracy_depth = accuracy_score(y_test, y_pred_depth)\n",
    "    print(f\"  max_depth={depth}: Accuracy = {accuracy_depth:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Train a Bagging Regressor using different base estimators (Decision Tree and K-Neighbors) and compare performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Bagging with Decision Tree Regressor\n",
    "bag_reg_dt = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
    "bag_reg_dt.fit(X_train, y_train)\n",
    "y_pred_dt_bag = bag_reg_dt.predict(X_test)\n",
    "mse_dt_bag = mean_squared_error(y_test, y_pred_dt_bag)\n",
    "print(f\"Bagging Regressor with Decision Tree MSE: {mse_dt_bag:.4f}\")\n",
    "\n",
    "# Bagging with K-Neighbors Regressor\n",
    "bag_reg_knn = BaggingRegressor(estimator=KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
    "bag_reg_knn.fit(X_train, y_train)\n",
    "y_pred_knn_bag = bag_reg_knn.predict(X_test)\n",
    "mse_knn_bag = mean_squared_error(y_test, y_pred_knn_bag)\n",
    "print(f\"Bagging Regressor with K-Neighbors MSE: {mse_knn_bag:.4f}\")\n",
    "\n",
    "if mse_dt_bag < mse_knn_bag:\n",
    "    print(\"Bagging Regressor with Decision Tree performed better.\")\n",
    "elif mse_knn_bag < mse_dt_bag:\n",
    "    print(\"Bagging Regressor with K-Neighbors performed better.\")\n",
    "else:\n",
    "    print(\"Both Bagging Regressors performed equally.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest Classifier\n",
    "rf_clf_auc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_auc.fit(X_train, y_train)\n",
    "\n",
    "# Get probability predictions for the positive class\n",
    "y_prob_auc = rf_clf_auc.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate and print the ROC-AUC score\n",
    "auc_score_rf = roc_auc_score(y_test, y_prob_auc)\n",
    "print(f\"Random Forest Classifier ROC-AUC Score: {auc_score_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Train a Bagging Classifier and evaluate its performance using cross-validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Initialize a Bagging Classifier\n",
    "bag_clf_cv = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(bag_clf_cv, X, y, cv=5, scoring=\'accuracy\')\n",
    "\n",
    "print(f\"Bagging Classifier Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
    "print(f\"Standard Deviation of Cross-Validation Accuracy: {cv_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Train a Random Forest Classifier and plot the Precision-Recall curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, PrecisionRecallDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train a Random Forest Classifier\n",
    "rf_clf_pr = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf_pr.fit(X_train, y_train)\n",
    "\n",
    "# Get probability predictions for the positive class\n",
    "y_prob_pr = rf_clf_pr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate precision and recall for various thresholds\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob_pr)\n",
    "\n",
    "# Plot the Precision-Recall curve\n",
    "disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
    "disp.plot()\n",
    "plt.title(\"Precision-Recall Curve for Random Forest Classifier\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=10, n_redundant=5, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define base estimators\n",
    "estimators_stack = [\n",
    "    (\"rf\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    (\"lr\", LogisticRegression(solver=\"liblinear\", random_state=42))\n",
    "]\n",
    "\n",
    "# Initialize Stacking Classifier with a final estimator\n",
    "st_clf_rf_lr = StackingClassifier(estimators=estimators_stack, final_estimator=LogisticRegression(solver=\"liblinear\", random_state=42), cv=5)\n",
    "\n",
    "# Train the Stacking Classifier\n",
    "st_clf_rf_lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_st_rf_lr = st_clf_rf_lr.predict(X_test)\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy_st_rf_lr = accuracy_score(y_test, y_pred_st_rf_lr)\n",
    "print(f\"Stacking Classifier (RF+LR) Accuracy: {accuracy_st_rf_lr:.4f}\")\n",
    "\n",
    "# Optionally, compare with individual base estimator accuracies\n",
    "print(\"\\nIndividual Base Estimator Accuracies:\")\n",
    "for name, estimator in estimators_stack:\n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred_base = estimator.predict(X_test)\n",
    "    acc_base = accuracy_score(y_test, y_pred_base)\n",
    "    print(f\"  {name.upper()} Accuracy: {acc_base:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Train a Bagging Regressor with different levels of bootstrap samples and compare performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, n_informative=10, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define different max_samples values to test\n",
    "max_samples_list = [0.5, 0.7, 0.9, 1.0] # 1.0 means use all samples with replacement\n",
    "\n",
    "print(\"Comparing Bagging Regressor MSE with different levels of bootstrap samples:\")\n",
    "for max_samples in max_samples_list:\n",
    "    bag_reg_samples = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, max_samples=max_samples, random_state=42)\n",
    "    bag_reg_samples.fit(X_train, y_train)\n",
    "    y_pred_samples = bag_reg_samples.predict(X_test)\n",
    "    mse_samples = mean_squared_error(y_test, y_pred_samples)\n",
    "    print(f\"  max_samples={max_samples}: MSE = {mse_samples:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

